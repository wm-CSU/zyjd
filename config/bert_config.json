{
  "model_type": "roberta",
  "max_seq_len": 510,
  "model_path": "model/chinese_roberta_wwm_ext/",
  "last_model_path": "model/chinese_roberta_wwm_ext/",
  "log_path": "log/",

  "train_file": "data/train.json",
  "test_file": "data/test_v1.txt",
  "test_to_file": "result/prediction.txt",

  "batch_size": 4,
  "multi_gpu": false,
  "use_whitening": false,

  "pooling": "first-last-avg",
  "hidden_size": 768,
  "num_classes": 148,
  "dropout": 0.01,
  "bilstm_num_layers": 1,

  "lr": 2e-5,
  "num_epoch": 100,
  "warmup_ratio": 0.05,
  "gradient_accumulation_steps": 1,
  "max_grad_norm": 1e-3,

  "experiment_name": "RoBERTa-with_resampling_lstm"
}